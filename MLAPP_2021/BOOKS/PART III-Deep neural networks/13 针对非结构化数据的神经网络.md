# 13 针对非结构化数据的神经网络

## 13.1 引言

在部分II， 我们讨论了在回归和分类任务中的线性模型。其中，在第11章，我们讨论了线性回归，即 $p(y|\mathbf{x},\mathbf{w})=\mathcal{N}(y|\mathbf{w}^{\rm{T}}\mathbf{x}, \sigma^2)$。在第10章，我们讨论了逻辑回归，在二分类任务中，模型定义为 $p(y|\mathbf{x}, \mathbf{w})={\rm{Ber}}(y|\sigma(\mathbf{w}^T\mathbf{x}))$，在多分类任务中，模型定义为 $p(y|\mathbf{x},\mathbf{w})={\rm{Cat}}(y|\mathcal{S}(\mathbf{Wx}))$。在第12章，我们讨论了广义线性模型，定义为:
$$
p(\mathbf{y}|\mathbf{x};\mathbf{\theta})=p(\mathbf{y}|g^{-1}(f(\mathbf{x};\mathbf{\theta}))) \tag{13.1}
$$
其中 $p(\mathbf{y}|\mathbf{\mu})$ 表示均值为 $\mathbf{\mu}$ 的指数族分布，$g^{-1}()$ 表示该指数族分布对应的逆连接函数。上式中:
$$
f(\mathbf{x};\mathbf{\theta})=\mathbf{Wx} + \mathbf{b} \tag{13.2}
$$
表示关于输入的一个线性（仿射）变换函数， 其中 $\mathbf{W}$ 被称为**权重** (weights)，$\mathbf{b}$ 被称为**偏置** (biases)。

线性关系的假设具有很强的局限性。为了增加此类线性模型的灵活性，一种简单方法是使用特征变换，即用 $\phi(\mathbf{x})$ 替代 $\mathbf{x}$。举例来说，我们可以使用多项式变换，在1维数据中，该变换定义为 $\phi(x)=[1,x,x^2,x^3,...]$，我们在 1.2.2.2 节中对该方法进行了讨论。这种方法有时被称为 **基函数拓展** (basis function expansion)。基于该变换，上述模型定义为:
$$
f(\mathbf{x}; \mathbf{\theta})=\mathbf{W}\mathbf{\phi}(\mathbf{x}) + \mathbf{b} \tag{13.3}
$$
上式关于参数 $\mathbf{\theta}=(\mathbf{W}, \mathbf{b})$ 依然是线性关系，从而降低了模型的拟合难度。然而，手动设计的特征变换函数通常具有很强的局限性。

一个很自然的拓展是为特征提取器赋予自己的参数 $\mathbf{\theta}^\prime$， 即:
$$
f(\mathbf{x};\mathbf{\theta},\mathbf{\theta}^\prime)=\mathbf{W}\phi(\mathbf{x};\mathbf{\theta}^\prime)+\mathbf{b} \tag{13.4}
$$
我们可以递归地重复上述过程，从而构造一个越来越复杂的函数。如果我们组合 $L$ 个函数，我们得到
$$
f(\mathbf{x};\mathbf{\theta})=f_L(f_{L-1}(...(f_1(\mathbf{x}))...)) \tag{13.5}
$$
其中 $f_l(\mathbf{x})=f(\mathbf{x};\mathbf{\theta}_l)$ 为层 $l$ 的函数。这便是**深度神经网络** (deep neural networks, DNNs) 背后的关键思想。

“ DNN”一词实际上包含了更大的模型系列，其中我们将可微函数组合到任何类型的DAG（有向无环图）中，将输入映射到输出。 式 （13.5）是链式DAG的最简单示例。 这被称为**前馈神经网络**（feedforward neural network, FFNN）或**多层感知器**（multilayer perceptron, MLP）。

MLP假定输入是一个维度固定的矢量，即 $\mathbf{x} \in \mathbb{R}^D$。 我们称此类数据为“**非结构化数据**” (unstructured data)，因为我们没有输入的形式进行任何假设。 但是，这使得该模型难以应用于具有可变大小或形状的输入。 在第14章中，我们讨论了**卷积神经网络**（convolutional neural networks, CNN），其设计用于处理可变大小的图像。 在第15章中，我们讨论了**递归神经网络**（recurrent neural networks, RNN），其设计用于处理可变大小的序列。 在第23章中，我们讨论了**图神经网络**（graph neural networks, GNN），其设计用于处理可变大小的图。 有关DNN的更多信息，请参见其他书籍，例如。

## 13.2 多层感知机

在第10.2.5节，我们表明**感知机** (perceptron) 就是逻辑回归的一个确定性版本。具体来说，它是一个具备如下形式映射:
$$
f(\mathbf{x};\mathbf{\theta})=\mathbb{I}(\mathbf{w}^{\rm{T}}\mathbf{x}+b\ge0)=H(\mathbf{w}^{\rm{T}}\mathbf{x}+b) \tag{13.6}
$$
其中 $H(a)$ 表示 **单位阶跃函数** (heaviside step function)， 又被称为 **线性阈值函数** (linear threshold function)。 由于该感知机的决策边界依然是线性的，所以其表达能力十分有限。1969年，马文·明斯基（Marvin Minsky）和西摩·帕尔特（Seymour Papert）出版了一本名为《 Perceptrons》 [MP69]的著名著作，其中他们给出了许多感知器无法解决的模式识别问题的例子。 在讨论如何解决问题之前，我们在下面举一个具体的例子。

![xor-heaviside](.\figures\xor-heaviside.png)

### 13.2.1 抑或问题

“感知器”书中最著名的例子之一就是XOR问题。 这里的目标是学习一个计算其两个二进制输入的异或的函数。 表13.1给出了该函数的真值表。 我们在图13.1a中可视化此功能。 显然，数据不是线性可分离的，因此感知器无法表示该映射。

但是，我们可以通过堆叠多个感知器来克服此问题。 这称为**多层感知器**（multilayer perceptron, MLP）。 例如，要解决XOR问题，我们可以使用图13.1b所示的MLP。 它由3个感知器组成，分别表示为$h_1$，$h_2$和$y$。 节点 $x$ 表示输入，节点 $1$表示常数项。 节点 $h_1$ 和 $h_2$ 被称为**隐藏单元** （hidden units），因为在训练数据中未观察到它们的值。

第一个隐藏单元通过使用设置的合理权重来计算 $h1 = x1 \and x2$。（此处 $\and$ 表示 $\rm{AND}$ 操作。）特别地，它的输入为 $x_1$和$x_2$，且均由1.0加权，同时具有 -1.5 的偏差项（这由权重为 -1.5 的 “线” 来自其值固定为 1 的虚拟节点实现）。 因此，如果 $x_1$ 和 $x_2$ 都打开，则 $h_1$ 将触发，因为
$$
\mathbf{w}_1^{\rm{T}}\mathbf{x}-b_1=[1.0, 1.0]^{\rm{T}}[1, 1] - 1.5 =0.5 > 0 \tag{13.7}
$$
类似的，第二个隐藏单元计算 $h_2=x_1 \or x_2$，其中 $\or$ 为 $\rm{OR}$ 操作，第三个节点计算输出 $y=\overline{h_1} \and h_2$，其中 $\bar{h}=\neg h$ 为 $\rm{NOT}$ 操作 （ 逻辑非）。 所以 $y$ 计算
$$
y=f(x_1,x_2)=\overline{(x_1 \and x_2)} \and (x_1 \or x_2) \tag{13.8}
$$
上式等价于 $\rm{XOR}$ 函数。

通过扩展上述示例，我们可以证明MLP可以表示任何逻辑函数。 但是，我们显然希望避免手动指定权重和偏差。 在本章的其余部分，我们将讨论从数据中学习这些参数的方法。

### 13.2.2 可微多层感知器

我们在第13.2.1节中讨论的MLP被定义为感知器的叠加，每个感知器都包含不可微的Heaviside函数。 这使得这种模型很难训练，这就是为什么它们从未被广泛使用的原因。然而，如果我们将阶跃函数 $H:\mathbb{R}\rightarrow \{0,1\}$ 替换为一个可微的 **激活函数** （activation function）$\varphi:\mathbb{R} \rightarrow \mathbb{R}$ 。更精确地讲，我们定义每一层 $l$ 的隐藏单元 $\mathbf{z}_l$ 为通过激活函数逐元素传递的上一层隐藏单元的线性变换：
$$
\mathbf{z}_l=f_l(\mathbf{z}_{l-1})=\varphi(\mathbf{b}_l+\mathbf{W}_l\mathbf{z}_{l-1}) \tag{13.9}
$$
或者，以标量的形式：
$$
z_{kl}=\varphi_l \left( b_{kl}+\sum_{j=1}^{K_{l-1}}w_{jkl}z_{jl-1} \right) \tag{13.10}
$$
如果我们现在将这些函数中的$L$个一起组成，如式（13.5）中所示。然后我们可以使用链式规则，也称为**反向传播** （backpropagation），计算每一层中的参数的梯度，如我们在第13.3节中所解释的。 （这对于任何一种可微分的激活函数都是正确的，尽管某些类型的函数要比其他类型的函数更好，正如我们在第13.2.3节中讨论的那样。）然后，我们可以将梯度传递给优化器，从而最小化某些训练目标，正如我们在13.4节讨论的那样。 因此，术语“ MLP”几乎总是指可微形式的模型，而不是指具有不可微分线性阈值单位的历史版本。

![sigmoid_saturation_plot](.\figures\sigmoid_saturation_plot.png)

![activationFuns2](.\figures\activationFuns2.png)

### 13.2.3 激活函数

我们可以在每一层使用任何一种可微的激活函数。然而，如果我们使用一个 *线性* 激活函数，$\varphi_l(a)=c_la$，那整个模型将退化为一个常规的线性模型。为了说明这一点，式 （13.5）将变成
$$
f(\mathbf{x};\mathbf{\theta})=\mathbf{W}_Lc_L(\mathbf{W}_{L-1}c_{L-1}(...(\mathbf{W}_1\mathbf{x})...)) \propto \mathbf{W}_L\mathbf{W}_{L-1}...\mathbf{W}_1\mathbf{x}=\mathbf{W}^\prime\mathbf{x} \tag{13.11}
$$
在上式中，为了符号上的简洁性，我们丢弃了偏置项。基于上述原因，使用非线性激活函数就显得十分重要。

在神经网络的早期发展阶段，一个常见的选择是使用 S 型 (logistic) 函数，该函数可以看做是单位阶跃函数的平滑近似版本。然而，如图13.2a所示，对于较大的正输入，S形函数的饱和值为1；对于较大的负输入，S形函数饱和值为0。 tanh函数具有相似的形状，但其饱和值为-1和+1。 在这些饱和区域，输出关于输入的斜率将接近于零，因此，如我们在第13.4.2节中所讨论的，来自深层的任何梯度信号都将“消失”。

要想训练一个非常深的神经网络模型，一个关键因素是使用**非饱和激活函数** （non-saturating activation functions）。几种不同的激活函数如表13.2所示。其中最常用的是**整流线性单元** （rectifled linear unit, ReLU）。定义为
$$
{\rm{ReLU}}(a)=\max(a,0)=a\mathbb{I}(a>0) \tag{13.12}
$$
该 ReLU 函数简单地将负值输入置零，并保持正值输入保持不变。如13.3.3.2节所介绍的，这种形式至少保证对于正值输入，梯度的值为1，从而避免了梯度的消失。

不幸的是，对于负值输入，ReLU的梯度为0，因此，该单元将永远无法获得任何反馈信号来帮助其摆脱此参数设置； 这称为“**垂死的ReLU**” (dying ReLU) 问题。

一种简单的解决方法是使用[MHN13]中提出的**泄漏ReLU** （leaky ReLU）。 定义为
$$
{\rm{LReLU}}(a;\alpha)=\max(\alpha a,a) \tag{13.13}
$$
其中 $0 \lt \alpha \lt 1$。该函数对于正值输入的斜率为1，对于负值输入的斜率为 $\alpha$， 所以可以确保当输入为负值时，依然可以有信号可以从更深的层中反传回来。如果我们允许参数 $\alpha$ 可以学习，而非固定， 泄露 ReLU将被称为 **参数化ReLU** （parametric ReLU）。

另一个广泛的选择是 [CUH16] 中提出的ELU，定义为
$$
{\rm{ELU}}(a;\alpha) = \begin{cases}
\alpha(e^a-1) & \text{if } a\le0\\
a & \text{if } a \gt 0
\end{cases} \tag{13.14}
$$
与漏泄ReLU相比，它具有平滑函数的优点。

在[Kla + 17]中提出了一种 ELU 的轻微变体，称为 SELU（自规范化ELU）。 形式为
$$
{\rm{SELU}}(a;\alpha,\lambda)=\lambda{\rm{ELU}}(a;\alpha) \tag{13.15}
$$
出乎意料的是，他们证明了通过设置和精心选择的值，即使不使用 batchnorm 技术（见13.4.5节），也可以确保通过激活功能来确保每个层的输出是被标准化的（假设输入也已标准化）。 这可以促进模型的拟合。

作为手动发现良好的激活函数的替代方法，我们可以使用黑盒优化方法来搜索函数形式的空间。 [RZL17]使用了这种方法，他们发现了称为**swish**的函数，该函数在某些图像分类基准上似乎做得很好。函数定义为
$$
{\rm{swish}}(a;\beta)=a\sigma(\beta a)\tag{13.16}
$$
有关这些函数的可视化对比，请参见图13.2b。 我们看到，它们主要是在处理负输入的方式上存在差异。
