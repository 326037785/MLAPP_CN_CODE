# 13 针对非结构化数据的神经网络

## 13.1 引言

在部分II， 我们讨论了在回归和分类任务中的线性模型。其中，在第11章，我们讨论了线性回归，即 $p(y|\mathbf{x},\mathbf{w})=\mathcal{N}(y|\mathbf{w}^{\rm{T}}\mathbf{x}, \sigma^2)$。在第10章，我们讨论了逻辑回归，在二分类任务中，模型定义为 $p(y|\mathbf{x}, \mathbf{w})={\rm{Ber}}(y|\sigma(\mathbf{w}^T\mathbf{x}))$，在多分类任务中，模型定义为 $p(y|\mathbf{x},\mathbf{w})={\rm{Cat}}(y|\mathcal{S}(\mathbf{Wx}))$。在第12章，我们讨论了广义线性模型，定义为:
$$
p(\mathbf{y}|\mathbf{x};\mathbf{\theta})=p(\mathbf{y}|g^{-1}(f(\mathbf{x};\mathbf{\theta}))) \tag{13.1}
$$
其中 $p(\mathbf{y}|\mathbf{\mu})$ 表示均值为 $\mathbf{\mu}$ 的指数族分布，$g^{-1}()$ 表示该指数族分布对应的逆连接函数。上式中:
$$
f(\mathbf{x};\mathbf{\theta})=\mathbf{Wx} + \mathbf{b} \tag{13.2}
$$
表示关于输入的一个线性（仿射）变换函数， 其中 $\mathbf{W}$ 被称为**权重** (weights)，$\mathbf{b}$ 被称为**偏置** (biases)。

线性关系的假设具有很强的局限性。为了增加此类线性模型的灵活性，一种简单方法是使用特征变换，即用 $\phi(\mathbf{x})$ 替代 $\mathbf{x}$。举例来说，我们可以使用多项式变换，在1维数据中，该变换定义为 $\phi(x)=[1,x,x^2,x^3,...]$，我们在 1.2.2.2 节中对该方法进行了讨论。这种方法有时被称为 **基函数拓展** (basis function expansion)。基于该变换，上述模型定义为:
$$
f(\mathbf{x}; \mathbf{\theta})=\mathbf{W}\mathbf{\phi}(\mathbf{x}) + \mathbf{b} \tag{13.3}
$$
上式关于参数 $\mathbf{\theta}=(\mathbf{W}, \mathbf{b})$ 依然是线性关系，从而降低了模型的拟合难度。然而，手动设计的特征变换函数通常具有很强的局限性。

一个很自然的拓展是为特征提取器赋予自己的参数 $\mathbf{\theta}^\prime$， 即:
$$
f(\mathbf{x};\mathbf{\theta},\mathbf{\theta}^\prime)=\mathbf{W}\phi(\mathbf{x};\mathbf{\theta}^\prime)+\mathbf{b} \tag{13.4}
$$
我们可以递归地重复上述过程，从而构造一个越来越复杂的函数。如果我们组合 $L$ 个函数，我们得到
$$
f(\mathbf{x};\mathbf{\theta})=f_L(f_{L-1}(...(f_1(\mathbf{x}))...)) \tag{13.5}
$$
其中 $f_l(\mathbf{x})=f(\mathbf{x};\mathbf{\theta}_l)$ 为层 $l$ 的函数。这便是**深度神经网络** (deep neural networks, DNNs) 背后的关键思想。

“ DNN”一词实际上包含了更大的模型系列，其中我们将可微函数组合到任何类型的DAG（有向无环图）中，将输入映射到输出。 式 （13.5）是链式DAG的最简单示例。 这被称为**前馈神经网络**（feedforward neural network, FFNN）或**多层感知器**（multilayer perceptron, MLP）。

MLP假定输入是一个维度固定的矢量，即 $\mathbf{x} \in \mathbb{R}^D$。 我们称此类数据为“**非结构化数据**” (unstructured data)，因为我们没有输入的形式进行任何假设。 但是，这使得该模型难以应用于具有可变大小或形状的输入。 在第14章中，我们讨论了**卷积神经网络**（convolutional neural networks, CNN），其设计用于处理可变大小的图像。 在第15章中，我们讨论了**递归神经网络**（recurrent neural networks, RNN），其设计用于处理可变大小的序列。 在第23章中，我们讨论了**图神经网络**（graph neural networks, GNN），其设计用于处理可变大小的图。 有关DNN的更多信息，请参见其他书籍，例如。

## 13.2 多层感知机

在第10.2.5节，我们表明**感知机** (perceptron) 就是逻辑回归的一个确定性版本。具体来说，它是一个具备如下形式映射:
$$
f(\mathbf{x};\mathbf{\theta})=\mathbb{I}(\mathbf{w}^{\rm{T}}\mathbf{x}+b\ge0)=H(\mathbf{w}^{\rm{T}}\mathbf{x}+b) \tag{13.6}
$$
其中 $H(a)$ 表示 **单位阶跃函数** (heaviside step function)， 又被称为 **线性阈值函数** (linear threshold function)。 由于该感知机的决策边界依然是线性的，所以其表达能力十分有限。1969年，马文·明斯基（Marvin Minsky）和西摩·帕尔特（Seymour Papert）出版了一本名为《 Perceptrons》 [MP69]的著名著作，其中他们给出了许多感知器无法解决的模式识别问题的例子。 在讨论如何解决问题之前，我们在下面举一个具体的例子。

![xor-heaviside](.\figures\xor-heaviside.png)

### 13.2.1 抑或问题

“感知器”书中最著名的例子之一就是XOR问题。 这里的目标是学习一个计算其两个二进制输入的异或的函数。 表13.1给出了该函数的真值表。 我们在图13.1a中可视化此功能。 显然，数据不是线性可分离的，因此感知器无法表示该映射。

但是，我们可以通过堆叠多个感知器来克服此问题。 这称为**多层感知器**（multilayer perceptron, MLP）。 例如，要解决XOR问题，我们可以使用图13.1b所示的MLP。 它由3个感知器组成，分别表示为$h_1$，$h_2$和$y$。 节点 $x$ 表示输入，节点 $1$表示常数项。 节点 $h_1$ 和 $h_2$ 被称为**隐藏单元** （hidden units），因为在训练数据中未观察到它们的值。

第一个隐藏单元通过使用设置的合理权重来计算 $h1 = x1 \and x2$。（此处 $\and$ 表示 $\rm{AND}$ 操作。）特别地，它的输入为 $x_1$和$x_2$，且均由1.0加权，同时具有 -1.5 的偏差项（这由权重为 -1.5 的 “线” 来自其值固定为 1 的虚拟节点实现）。 因此，如果 $x_1$ 和 $x_2$ 都打开，则 $h_1$ 将触发，因为
$$
\mathbf{w}_1^{\rm{T}}\mathbf{x}-b_1=[1.0, 1.0]^{\rm{T}}[1, 1] - 1.5 =0.5 > 0 \tag{13.7}
$$
类似的，第二个隐藏单元计算 $h_2=x_1 \or x_2$，其中 $\or$ 为 $\rm{OR}$ 操作，第三个节点计算输出 $y=\overline{h_1} \and h_2$，其中 $\bar{h}=\neg h$ 为 $\rm{NOT}$ 操作 （ 逻辑非）。 所以 $y$ 计算
$$
y=f(x_1,x_2)=\overline{(x_1 \and x_2)} \and (x_1 \or x_2) \tag{13.8}
$$
上式等价于 $\rm{XOR}$ 函数。

通过扩展上述示例，我们可以证明MLP可以表示任何逻辑函数。 但是，我们显然希望避免手动指定权重和偏差。 在本章的其余部分，我们将讨论从数据中学习这些参数的方法。

### 13.2.2 可微多层感知器

我们在第13.2.1节中讨论的MLP被定义为感知器的叠加，每个感知器都包含不可微的Heaviside函数。 这使得这种模型很难训练，这就是为什么它们从未被广泛使用的原因。然而，如果我们将阶跃函数 $H:\mathbb{R}\rightarrow \{0,1\}$ 替换为一个可微的 **激活函数** （activation function）$\varphi:\mathbb{R} \rightarrow \mathbb{R}$ 。更精确地讲，我们定义每一层 $l$ 的隐藏单元 $\mathbf{z}_l$ 为通过激活函数逐元素传递的上一层隐藏单元的线性变换：
$$
\mathbf{z}_l=f_l(\mathbf{z}_{l-1})=\varphi(\mathbf{b}_l+\mathbf{W}_l\mathbf{z}_{l-1}) \tag{13.9}
$$
或者，以标量的形式：
$$
z_{kl}=\varphi_l \left( b_{kl}+\sum_{j=1}^{K_{l-1}}w_{jkl}z_{jl-1} \right) \tag{13.10}
$$
如果我们现在将这些函数中的$L$个一起组成，如式（13.5）中所示。然后我们可以使用链式规则，也称为**反向传播** （backpropagation），计算每一层中的参数的梯度，如我们在第13.3节中所解释的。 （这对于任何一种可微分的激活函数都是正确的，尽管某些类型的函数要比其他类型的函数更好，正如我们在第13.2.3节中讨论的那样。）然后，我们可以将梯度传递给优化器，从而最小化某些训练目标，正如我们在13.4节讨论的那样。 因此，术语“ MLP”几乎总是指可微形式的模型，而不是指具有不可微分线性阈值单位的历史版本。

![sigmoid_saturation_plot](.\figures\sigmoid_saturation_plot.png)

![activationFuns2](.\figures\activationFuns2.png)

### 13.2.3 激活函数

我们可以在每一层使用任何一种可微的激活函数。然而，如果我们使用 *线性* （linear）激活函数  $\varphi_l(a)=c_la$，那整个模型将退化为一个常规的线性模型。以式 （13.5）为例，该模型将退化为：
$$
f(\mathbf{x};\mathbf{\theta})=\mathbf{W}_Lc_L(\mathbf{W}_{L-1}c_{L-1}(...(\mathbf{W}_1\mathbf{x})...)) \propto \mathbf{W}_L\mathbf{W}_{L-1}...\mathbf{W}_1\mathbf{x}=\mathbf{W}^\prime\mathbf{x} \tag{13.11}
$$
在上式中，为了符号上的简洁性，我们丢弃了偏置项。基于上述原因，使用非线性激活函数就显得十分重要。

在神经网络的早期发展阶段，一个常见的选择是使用 S 型 (logistic) 激活函数，该函数可以看做是单位阶跃函数的平滑近似版本。然而，如图13.2a所示，对于较大的正值输入，S 形函数存在饱和值 1；对于较大的负值输入，S 形函数存在饱和值 0。 tanh激活函数具有相似的形状，但其饱和值分别为 -1 和 +1。 在这些饱和区域，输出关于输入的斜率将接近于零。因此，如我们在第13.4.2节中所讨论的，来自深层网络的任何梯度信号都将“消失”。

要想成功训练一个非常深的神经网络模型，一个关键因素是使用 **非饱和激活函数** （non-saturating activation functions）。几种不同的激活函数如表13.2所示。其中最常用的是 **整流线性单元** （rectifled linear unit, ReLU）。定义为
$$
{\rm{ReLU}}(a)=\max(a,0)=a\mathbb{I}(a>0) \tag{13.12}
$$
该 ReLU 函数简单地将负值输入置零，并保持正值输入保持不变。如13.3.3.2节所介绍的，这种形式至少保证对于正值输入，梯度的值为1，从而避免了梯度的消失。

不幸的是，对于负值输入，ReLU的梯度依然为0，因此，该单元将永远无法获得任何反馈信号来帮助其摆脱当前的参数设置 （**译者注：**即无法逃离负值区域）； 这被称为 “**垂死ReLU**” (dying ReLU) 问题。

一种简单的解决方法是使用[MHN13]中提出的 **泄漏ReLU** （leaky ReLU）。 定义为
$$
{\rm{LReLU}}(a;\alpha)=\max(\alpha a,a) \tag{13.13}
$$
其中 $0 \lt \alpha \lt 1$。该函数对于正值输入的斜率为 1，对于负值输入的斜率为 $\alpha$， 所以可以确保当输入为负值时，依然可以有信号可以从更深的网络层中反传回来。如果我们允许参数 $\alpha$ 可以学习，而非固定， 泄露 ReLU将被称为 **参数化ReLU** （parametric ReLU）。

另一个广泛的选择是 [CUH16] 中提出的ELU，定义为
$$
{\rm{ELU}}(a;\alpha) = \begin{cases}
\alpha(e^a-1) & \text{if } a\le0\\
a & \text{if } a \gt 0
\end{cases} \tag{13.14}
$$
与漏泄ReLU相比，它具有平滑函数的优点。

在[Kla + 17]中提出了一种 ELU 的轻微变体，称为 SELU（自规范化ELU）。 形式为
$$
{\rm{SELU}}(a;\alpha,\lambda)=\lambda{\rm{ELU}}(a;\alpha) \tag{13.15}
$$
出乎意料的是，他们证明了通过为 $\alpha$ 和 $\lambda$ 设置精心选择的值，即使不使用 batchnorm 技术（见13.4.5节），也可以确保通过激活函数来确保每个网络层的输出是被标准化的（假设输入也已标准化）。 这可以促进模型的拟合。

作为手动发现良好的激活函数的替代方法，我们可以使用黑盒优化方法来对激活函数空间进行搜索。 [RZL17]使用这种方法发现了称为 **swish** 的函数，该函数在某些图像分类数据集上似乎表现很好。该函数定义为
$$
{\rm{swish}}(a;\beta)=a\sigma(\beta a)\tag{13.16}
$$
有关这些函数的可视化对比，请参见图13.2b。 我们看到，它们主要是在处理负输入的方式上存在差异。

### 13.2.4 案例模型

MLPs可以对很多类型的数据进行分类和回归，接下来我们将给出一些案例。

#### 13.2.4.1 MLP用于表格数据分类

图13.3给出了一个包含两个隐藏层的MLP示意图，将该MLP应用于1.2.1.1节中的表格鸢尾花数据集，该数据集具有4个特征和3个类别。 该模型具有如下形式
$$
\begin{align}
p(y|\mathbf{x};\mathbf{\theta})=& {\rm{Cat}}(y|f_3(\mathbf{x};\mathbf{\theta})) \tag{13.17}\\
f_3(\mathbf{x};\mathbf{\theta})=&\mathcal{S}(\mathbf{W}_3f_2(\mathbf{x};\mathbf{\theta})+\mathbf{b}_3) \tag{13.18} \\
f_2(\mathbf{x};\mathbf{\theta})=&\varphi_2(\mathbf{W}_2f_1(\mathbf{x};\mathbf{\theta})+\mathbf{b}_2) \tag{13.19} \\
f_1(\mathbf{x};\mathbf{\theta})=&\varphi_1(\mathbf{W}_1f_0(\mathbf{x};\mathbf{\theta})+\mathbf{b}_1) \tag{13.19} \\
f_0(\mathbf{x};\mathbf{\theta})=&\mathbf{x} \tag{13.21}
\end{align}
$$
其中 $\mathbf{\theta}=(\mathbf{W}_3,\mathbf{b}_3,\mathbf{W}_2,\mathbf{b}_2,\mathbf{W}_1,\mathbf{b}_1)$ 为模型中的参数，对应于 3 组可调节权重的连接边。我们看到最终（输出）层的激活函数为softmax函数，softmax函数是分类分布的反向连接函数。对于隐藏层，我们可以自由选择所需的不同形式的激活函数，正如我们在第13.2.3节中所讨论的。

#### 13.2.4.2 MLP用于图像分类

要将MLP应用于图像分类，我们需要将2d输入“**展开**”（flatten）为1d向量。 然后，我们可以使用类似于第13.2.4.1节中所述的前馈网络。 例如，考虑构建一个MLP以对MNIST数字进行分类（第3.7.2节）。 这些数字表示为 $28\times28 = 784$ 维向量。 如果我们使用2个具有128个单位的隐藏层，和1个包含10个输出单元的softmax层，将得到如图13.4所示的模型。

我们在图13.5中展示了一些该模型的预测结果。 我们对训练集仅训练两个“**周期**”（epochs, 遍历数据集的次数），但是该模型已经具备较好的性能，测试集的准确率为 97.1％。 此外，错误的预测案例似乎也是可以理解的，例如将9误认为是3。训练更多的时间可以进一步提高测试的准确性。

在第14章中，我们讨论了另一种称为卷积神经网络的模型，该模型更适用于图像数据的处理。 通过利用与图像数据相关的空间结构的先验知识，它可以获得更好的性能，并使用更少的参数。相比之下，MLP对输入的排列具有不变性。 换句话说，我们可以随机地对像素进行排列，并且可以获得相同的结果（前提是我们对所有的输入使用相同的随机排列算法）。

#### 13.2.4.3 MLP用于电影评论的情感分析

[Maa + 11]的IMDB电影评论数据集。 （IMDB代表“互联网电影数据库”。）被称为 “文本分类的MNIST”。该数据集包含25k带有标签的样本用于训练，而25k的样本用于测试。 每个样本都有一个二进制标签，代表积极或消极的评分。 此任务称为（二进制）**情感分析** （sentiment analysis）。 例如，以下是训练集中的两个样本：

1. this film was just brilliant casting location scenery story direction everyone’s really suited the part they played robert <UNK> is an amazing actor ...
2. big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i’ve seen hundreds...

毫不奇怪，第一个样本被标记为正例，第二个标记为负例。

我们可以设计一个MLP来进行情感分析，如下所示。假设输入是一个包含 $T$ 个符号的序列 $\mathbf{x}_{1:T}$，其中 $\mathbf{x}_t$ 是一个长度为 $V$ 的 one-hot 向量，其中 $V$ 为词语料库的大小。我们将此视为无序的单词袋（第10.4.3.1节）。模型的第一层为 $E\times V$  的嵌入矩阵 $\mathbf{W}_1$，该层将每一个稀疏的 $V$ 维向量映射到一个稠密的 $E$ 维向量 ${\rm{e}}_t=\mathbf{W}_1\mathbf{x}_t$（见19.5节学习更多关于词嵌入的细节）。接着我们使用 **全局平均池化**（global average pooling）将 $T\times D$ 的序列嵌入向量转化为一个固定长度的向量 ${\rm{\bar{\mathbf{e}}}}=\frac{1}{T}\sum_{t=1}^T{\rm{\mathbf{e}}}_t$。接着我们将该向量传入一个非线性隐藏层，计算一个 $K$ 维向量 $\mathbf{h}$，并将其传入最后的线性 logistic 层。 综上所述，模型定义如下：
$$
\begin{align}
p(y|\mathbf{x};\mathbf{\theta}) = & {\rm{Ber}}(y|\sigma(\mathbf{w}_3^{\rm{T}}\mathbf{h}+b_3)) \tag{13.22} \\
\mathbf{h}= & \varphi(\mathbf{W}_2{\rm{\bar{\mathbf{e}}}}+\mathbf{b}_2) \tag{13.23} \\
{\bar{\mathbf{e}}}=& \frac{1}{T}\sum_{t=1}^T\mathbf{e}_t \tag{13.24} \\
\mathbf{e}_t=& \mathbf{W}_1\mathbf{x}_t \tag{13.25}
\end{align}
$$
如果我们使用的语料库大小为 $V = 1000$，嵌入向量维度为 $E = 16$，隐藏层的维度为16，则得到的模型如图13.6所示。 模型在验证集的准确度为86％。

我们看到模型中大多数参数都分布在嵌入矩阵中，这可能会导致过拟合问题。 幸运的是，正如我们在第19.5节中讨论的那样，我们可以执行词嵌入模型的无监督预训练，然后我们只需要微调此特定标记任务的输出层参数即可。

#### 13.2.4.4 MLP用于异方差回归

我们还可以使用MLP进行回归。 图13.7显示了如何为异方差 （heteroskedastic）非线性回归任务建立模型。 （术语“异方差”仅表示预测的输出方差与输入有关，如第3.3.3节中所述。）该函数具有两个输出，分别表示 $f_\mu(\mathbf{x})=\mathbb{E}[y|\mathbf{x},\mathbf{\theta}]$ 和 $f_\sigma(\mathbf{x})=\sqrt{\mathbb{V}[y|\mathbf{x},\mathbf{\theta}]}$。如图13.7所示，通过使用一个共享的“**主干网络**”（backbone）和两个输出“**头**”（heads）， 我们可以在这两个函数之间共享大多数层（因此也可以共享参数）。对于 $\mu$ 头，我们使用一个线性激活函数 $\varphi(a)=a$。对于 $\sigma$ 头，我们使用 softplus 激活函数 $\varphi(a)=\sigma_{+}(a)$。如果我们使用线性头和一个非线性主干网络，整个模型定义为：
$$
p(y|\mathbf{x},\mathbf{\theta})=\mathcal{N}(y|\mathbf{w}_\mathbf{\mu}^{\rm{T}}f(\mathbf{x};\mathbf{w}_{\rm{shared}}), \sigma_{+}(\mathbf{w}_{\mathbf{\sigma}}^{\rm{T}}f(\mathbf{x};\mathbf{w}_{\rm{shared}}))) \tag{13.26}
$$
图13.8显示了这种模型在某些数据集上的优势，在该数据集中，预测的期望值随时间线性增长，并且随季节波动，与此同时，数据的方差呈二次方增加趋势。（这是 **随机波动率模型** （stochastic volatility model）的一个简单示例；它可以用于对财务数据以及地球的全球温度进行建模，其中地球温度的（由于气候变化）均值和方差不断增加。）我们发现 将输出方差 $\sigma^2$ 视为固定（与输入无关）参数的回归模型置信度有时会比较低，因为模型需要适应整体的噪声水平，并且无法适应输入空间中每个点的噪声水平。

### 13.2.5 深度的重要性

研究表明包含一个隐藏层的MLP是一个**通用函数逼近器**（universal function approximator），这意味着只要给定足够的隐藏单元，MLP就可以逼近任何平滑函数，并达到任何所需的精度水平[HSW89; cyb89; Hor91]。直观地讲，这样做的原因是每个隐藏的单元都可以指定一个半平面，并且这些单元的足够大的组合可以“划分”空间的任何区域，我们可以将其与任何响应相关联（这在分段使用时最容易看到 线性激活函数，如图13.9所示。

但是，实验和理论上的各种论证（例如[Has87; Mon + 14; Rag + 17; Pog + 17]）都表明，深层网络比浅层网络更有效。 原因是更高的层次可以利用先前的层次所学习的功能。 也就是说，该功能是以组合或分层的方式定义的。 例如，假设我们要对DNA字符串进行分类，并且正类与正则表达式* AA ?? CGCG ?? AA *相关联。 尽管我们可以将其与单个隐藏层模型配合使用，但是从直观上来说，如果模型首先学会使用第1层中的隐藏单元来检测AA和CG“基元”，然后使用这些功能来定义一个简单的模型，则将更容易学习 第2层中的线性分类器，类似于我们如何解决第13.2.1节中的XOR问题。

#### 13.2.5.1 深度学习革命

尽管DNN背后的思想可以追溯到几十年前，但直到2010年代，它们才开始被广泛使用。 突破性的时刻发生在2012年，当时[KSH12]表明深层的CNN可以在具有挑战性的ImageNet图像分类基准上显着提高性能，将错误率从一年的26％降低到16％（见图14.14b）； 与之前每年约减少2％的进度相比，这是一个巨大的飞跃。 大约在同一时间，[DHK13]表明，在各种语音识别任务上，深度神经网络可以大大优于现有技术。

DNN的使用中的“爆炸”有几个促成因素。 一是便宜的GPU（图形处理单元）的可用性。 它们最初是为了加快视频游戏的图像渲染速度而开发的，但是它们也可以大大减少适合大型CNN的时间，而大型CNN涉及类似的矩阵矢量计算。 另一个是大型标记数据集的增长，这使我们能够在不过度拟合的情况下将具有许多参数的复杂函数逼近器拟合。 （例如，ImageNet具有130万个带标签的图像，并用于拟合具有数百万个参数的模型。）的确，如果将深度学习系统视为“火箭”，那么大型数据集就被称为燃料。

由于DNN取得了巨大的经验成功，许多公司开始对该技术产生兴趣。 这导致开发了高质量的开源软件库，例如Tensorflow（由Google制造），PyTorch（由Facebook制造）和MXNet（由亚马逊制造）。 这些库支持复杂的微分函数的自动微分（请参阅第13.3节）和基于可伸缩的基于梯度的优化（请参见第5.4节）。 在本书的各个地方，我们将使用其中的一些库来实现各种模型，而不仅仅是DNN。

有关“深度学习革命”历史的更多详细信息，请参见[Sej18]。

### 13.2.6 与生物学的联系

在本节中，我们讨论了上文讨论过的各种神经网络（称为人工神经网络或ANN）与实际神经网络之间的联系。 真正的生物大脑如何工作的细节非常复杂（例如，参见[Kan + 12]），但是我们可以给出一个简单的“卡通”。

我们首先考虑单个神经元的模型。 对于第一近似，我们可以说神经元k是否发射，用hk 2 f0表示； 1g取决于其输入的活动（用x 2 RD表示）以及传入连接的强度（我们用wk 2 RD表示）。 我们可以使用ak = w> k x来计算输入的加权和。 这些权重可以看作是将输入xd连接到神经元hk的“电线”。 这些类似于真实神经元中的树突（见图13.10）。然后将该加权总和与阈值bk进行比较，如果激活超过阈值，则神经元触发； 这类似于神经元发出电输出或动作电位。 因此，我们可以使用hk（x）= H（w> k x􀀀bk）来建模神经元的行为，其中H（a）= I（a> 0）是Heaviside函数。 称为神经元的McCulloch-Pitts模型，并于1943年提出[MP43]。

我们可以将多个这样的神经元组合在一起以构成一个人工神经网络。 结果有时被视为大脑的模型。 但是，人工神经网络在许多方面与生物大脑不同，包括以下方面：

- 大多数ANN使用反向传播来修改其连接强度（请参阅第13.3节）。 但是，真正的大脑不会使用反向支撑，因为无法沿着轴突向后发送信息[Ben + 15b; BS16； KH19]。 相反，他们使用本地更新规则来调整突触强度。
- 大多数ANN都是严格的前馈，但是真实的大脑有很多反馈连接。 可以相信，这种反馈的作用类似于先验，可以与来自感官系统的自下而上的可能性结合起来，计算出世界上隐藏状态的后验，然后可以将其用于最佳决策（例如，参见[Doy + 07]）。
- 大多数人工神经网络使用简化的神经元，该神经元由经过非线性处理的加权总和组成，但实际的生物神经元具有复杂的树状树结构（见图13.10），具有复杂的时空动态。
- 大多数人工神经网络的大小和连接数均小于生物大脑（见图13.11）。 当然，在各种新型硬件加速器（例如GPU和TPU（张量处理单元）等）的推动下，人工神经网络每周都会变得越来越大。但是，即使人工神经网络在单元数量上与生物大脑相匹配，这种比较也具有误导性，因为 生物神经元的处理能力远高于人工神经元（见上文）。
- 大多数ANN被设计为对单个函数建模，例如将图像映射到标签，或将单词序列映射到另一个单词序列。 相比之下，生物大脑是非常复杂的系统，由多个专门的交互模块组成，这些模块实现不同种类的功能或行为，例如感知，控制，记忆，语言等（请参见[Sha88; Kan + 12]）。

当然，我们正在努力建立逼真的生物大脑模型（例如，蓝脑计划[Mar06; Yon19]）。但是，一个有趣的问题是，以这种详细程度研究大脑是否对“解决AI”有用？通常认为，生物大脑的低级细节并不重要，我们的目标是否是建造“智能机器”，就像飞机不会拍打自己的飞机一样。
翅膀。但是，大概“ AI”将遵循与智能生物代理类似的“智能定律”，就像飞机和鸟类遵循相同的空气动力学定律一样。
不幸的是，我们尚不知道什么是“智力法则”，或者甚至是否存在这样的法则。在本书中，我们假设任何智能主体都应遵循信息处理和贝叶斯决策理论的基本原理，这是在不确定性下做出决策的最佳方法（请参见第8.4.2节）。
当然，生物因子受到许多约束（例如，计算，生态），这通常需要算法“捷径”才能获得最佳解决方案。这可以解释人们在日常推理中使用的许多启发式方法。 GTA00; Gri20]。随着我们希望机器解决的任务变得越来越困难，我们也许能够从神经科学和认知科学的其他领域获得见识（例如，参见[MWK16; Has + 17; Lak + 17]）。
